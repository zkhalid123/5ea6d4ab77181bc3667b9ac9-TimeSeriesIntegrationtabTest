{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***GENERATED CODE FOR TimeSeriesIntegrationtabTest PIPELINE***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CONNECTOR FUNCTIONS TO READ DATA FROM DATABRICKS FILESYSTEM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import logging\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "logging.basicConfig(format='%(levelname)s:%(message)s', level=logging.INFO)\n",
    "\n",
    "\n",
    "class DBFSConnector:\n",
    "\n",
    "    def fetch(inStages, inStagesData, stageId, spark, config):\n",
    "        df = spark.read.\\\n",
    "            options(header='true' if eval(config)[\"is_header\"] == \"Use Header Line\" else 'false',\n",
    "                    inferschema='true',\n",
    "                    delimiter=eval(config)[\"delimiter\"])\\\n",
    "            .csv(eval(config)['url'])\n",
    "        display(df.limit(2).toPandas())\n",
    "        return df\n",
    "\n",
    "    def put(inStages, inStagesData, stageId, spark, config):\n",
    "        return inStagesData.write.format('csv').options(header='true' if eval(config)[\"is_header\"] == \"Use Header Line\" else 'false',\n",
    "                                                        delimiter=eval(config)[\"delimiter\"]).save((\"%s %s\") % (datetime.datetime.now().strftime(\"%Y-%m-%d %H.%M.%S\")+\"_\", eval(config)['url']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TRANSFORMATIONS FUNCTIONS THAT WILL BE APPLIED ON DATA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.NamingNPathAccord import NumtraNamingNPathAccord\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "from os import listdir\n",
    "from pyspark.sql.functions import col, when\n",
    "from pyspark.sql.functions import mean, stddev, min, max, col\n",
    "\n",
    "\n",
    "class CleanseData:\n",
    "    # def __init__(self,df):\n",
    "    #     #print()\n",
    "\n",
    "    def replaceByMean(self, feature, df, mean_=-1):\n",
    "\n",
    "        meanValue = df.select(mean(col(feature.name)).alias(\n",
    "            'mean')).collect()[0][\"mean\"]\n",
    "        df.fillna(meanValue, subset=[feature.name])\n",
    "        df.withColumn(feature.name, when(col(feature.name) == \" \",\n",
    "                                         meanValue).otherwise(col(feature.name).cast(\"Integer\")))\n",
    "        return df\n",
    "\n",
    "    def replaceByMax(self, feature, df, max_=-1):\n",
    "        maxValue = df.select(max(col(feature.name)).alias('max')).collect()[\n",
    "            0][\"max\"]\n",
    "        df.fillna(maxValue, subset=[feature.name])\n",
    "        df = df.withColumn(feature.name,\n",
    "                           when(col(feature.name) == \" \", maxValue).otherwise(col(feature.name)))\n",
    "        return df\n",
    "\n",
    "    def replaceByMin(self, feature, df, min_=-1):\n",
    "        minValue = df.select(min(col(feature.name)).alias('min')).collect()[\n",
    "            0][\"min\"]\n",
    "        df.fillna(minValue, subset=[feature.name])\n",
    "        df = df.withColumn(feature.name,\n",
    "                           when(col(feature.name) == \" \", minValue).otherwise(col(feature.name)))\n",
    "        return df\n",
    "\n",
    "    def replaceByStandardDeviation(self, feature, df, stddev_=-1):\n",
    "        stddevValue = df.select(stddev(col(feature.name)).alias(\n",
    "            'stddev')).collect()[0][\"stddev\"]\n",
    "        df.fillna(stddevValue, subset=[feature.name])\n",
    "        df = df.withColumn(feature.name,\n",
    "                           when(col(feature.name) == \" \", stddevValue).otherwise(col(feature.name)))\n",
    "        return df\n",
    "\n",
    "    def replaceDateRandomly(self, feature, df):\n",
    "        fillValue = df.where(col(feature.name).isNotNull()\n",
    "                             ).head(1)[0][feature.name]\n",
    "        df.fillna(str(fillValue), subset=[feature.name])\n",
    "        df = df.withColumn(feature.name,\n",
    "                           when(col(feature.name) == \" \", fillValue).otherwise(col(feature.name)))\n",
    "        # print(\"CleanseData:replaceDateRandomly Schema : \", df.#printSchema())\n",
    "        return df\n",
    "\n",
    "    def replaceNullValues(self, fList, df):\n",
    "        featuresList = df.schema.fields\n",
    "        for featureObj in fList:\n",
    "            for feat in featuresList:\n",
    "                if featureObj[\"feature\"] in feat.name:\n",
    "                    featureName = feat\n",
    "                    if \"mean\" in featureObj[\"replaceby\"]:\n",
    "                        df = self.replaceByMean(featureName, df)\n",
    "                    elif \"max\" in featureObj[\"replaceby\"]:\n",
    "                        df = self.replaceByMax(featureName, df)\n",
    "                    elif \"min\" in featureObj[\"replaceby\"]:\n",
    "                        df = self.replaceByMin(featureName, df)\n",
    "                    elif \"stddev\" in featureObj[\"replaceby\"]:\n",
    "                        df = self.replaceByStandardDeviation(featureName, df)\n",
    "                    elif \"random\" in featureObj[\"replaceby\"]:\n",
    "                        df = self.replaceDateRandomly(featureName, df)\n",
    "        return df\n",
    "\n",
    "\n",
    "Forecasting_FE_Methods = {\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "class TransformationTimeSeriesForecastingMain:\n",
    "    # TODO: change df argument in run with following\n",
    "    def run(inStages, inStagesData, stageId, spark, config):\n",
    "        configObj = json.loads(config)\n",
    "        featureData = configObj[\"FE\"]['featureList']\n",
    "        ForecastFE = configObj[\"FE\"]\n",
    "        transformationDF = inStagesData[inStages[0]]\n",
    "        featuresSelectedList = [ForecastFE['features']\n",
    "                                ['timecolumn'], ForecastFE['features']['tocompare']]\n",
    "        transformedDF = transformationDF.select(\n",
    "            [c for c in transformationDF.columns if c in featuresSelectedList])\n",
    "        transformedDF = CleanseData().replaceNullValues(featureData, transformedDF)\n",
    "        for transformation in featureData:\n",
    "            feature = transformation[\"feature\"]\n",
    "            if feature in featuresSelectedList:\n",
    "                if transformation[\"transformation\"] != '' and transformation[\"selected\"].lower() == \"true\" and not(feature.__contains__(\"_transform\")):\n",
    "                    transformedDF = Feature_Transformations_Methods[\"%s\" % transformation[\"transformation\"]](\n",
    "                        transformedDF, transformation)\n",
    "\n",
    "        df = getPandasDF(transformedDF, stageId,\n",
    "                         ForecastFE['features']['timecolumn'])\n",
    "        if \"statFunction\" in ForecastFE:\n",
    "            statFunction = ForecastFE['statFunction']\n",
    "            if 'Original' in statFunction[\"function\"]:\n",
    "                pass\n",
    "            elif \"parameter\" in statFunction:\n",
    "                df = Forecasting_FE_Methods[\"%s\" % statFunction[\"function\"]](\n",
    "                    df, statFunction['parameter'])\n",
    "            else:\n",
    "                df = Forecasting_FE_Methods[\"%s\" %\n",
    "                                            statFunction[\"function\"]](df)\n",
    "\n",
    "        return df\n",
    "\n",
    "\n",
    "def getPandasDF(transformedDF, stageID, timeFeature):\n",
    "    filepath = NumtraNamingNPathAccord().getExtraFilePath(stageID)\n",
    "    transformedDF.repartition(1).write.csv(\n",
    "        path=(filepath), mode=\"append\", header=\"true\")\n",
    "    CSV_Files = [file for file in listdir(\n",
    "        NumtraNamingNPathAccord().attachDBFSToPath(filepath)) if file.endswith('.csv')]\n",
    "    df = pd.read_csv(NumtraNamingNPathAccord().attachDBFSToPath(\n",
    "        filepath) + \"/\" + CSV_Files[0], delimiter=',', index_col=[timeFeature], encoding='utf-8')\n",
    "    if os.path.exists(NumtraNamingNPathAccord().attachDBFSToPath(filepath) + \"/\" + CSV_Files[0]):\n",
    "        os.remove(NumtraNamingNPathAccord().attachDBFSToPath(\n",
    "            filepath) + \"/\" + CSV_Files[0])\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**AUTOML FUNCTIONS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fbprophet import Prophet\n",
    "\n",
    "\n",
    "def driverProphet(df):\n",
    "    m = Prophet()\n",
    "    m.fit(df)\n",
    "    future = m.make_future_dataframe(periods=365)\n",
    "    forecast = m.predict(future)\n",
    "    forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail()\n",
    "    m.plot(forecast)\n",
    "\n",
    "\n",
    "return forecast\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**READING DATAFRAME**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "try: \n",
    "\tTimeSeriesIntegrationtabTest_DBFS = DBFSConnector.fetch([], {}, \"5ea6d4ab77181bc3667b9aca\", spark, \"{'url': '/Demo/example_wp_log_peyton_manning.csv', 'file_type': 'Delimeted', 'delimiter': ',', 'is_header': 'Use Header Line'}\")\n",
    "\n",
    "except Exception as ex: \n",
    "\tlogging.error(ex)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TRANSFORMING DATAFRAME**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: \n",
    "\tTimeSeriesIntegrationtabTest_FeatureForecast = TranformationsFeatureForecasting.TramformationTimeSeriesForecastingMain.run([\"5ea6d4ab77181bc3667b9aca\"],{\"5ea6d4ab77181bc3667b9aca\": TimeSeriesIntegrationtabTest_DBFS}, \"5ea6d4ab77181bc3667b9acb\", spark,json.dumps( {\"FE\": {\"functionList\": [{\"function\": \"log\"}, {\"function\": \"difference\"}, {\"function\": \"Original\"}], \"featureList\": [{\"transformationsData\": {}, \"feature\": \"Date\", \"type\": \"date\", \"selected\": \"True\", \"replaceby\": \"random\", \"stats\": {\"count\": \"\", \"mean\": \"\", \"stddev\": \"\", \"min\": \"\", \"max\": \"\", \"missing\": \"0\"}, \"transformation\": \"\"}, {\"transformationsData\": {}, \"feature\": \"WebPageViews\", \"type\": \"real\", \"selected\": \"True\", \"replaceby\": \"mean\", \"stats\": {\"count\": \"567\", \"mean\": \"8.16\", \"stddev\": \"0.85\", \"min\": \"6.79122146272619\", \"max\": \"12.846746888829\", \"missing\": \"0\"}, \"transformation\": \"\"}], \"originalfile\": \"/Demo/example_wp_log_peyton_manning.csv\", \"features\": {\"timecolumn\": \"Date\", \"tocompare\": \"WebPageViews\"}, \"dataPercentage\": \"100\", \"statFunction\": {\"function\": \"Original\", \"parameter\": \"\"}}}))\n",
    "\n",
    "except Exception as ex: \n",
    "\tlogging.error(ex)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TRAIN MODEL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: \n",
    "\tTimeseriesforecastml = driverProphet([TimeSeriesIntegrationtabTest_FeatureForecast])\n",
    "\n",
    "except Exception as ex: \n",
    "\tlogging.error(ex)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
